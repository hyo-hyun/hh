{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7b1661-d309-43db-aae7-07c762e065eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4d717e-cce5-441d-a706-2a3cdb0fbdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itwill\\Desktop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\itwill\\Desktop')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "485594ca-effa-4a71-9d50-2968dd61cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_taka=pd.read_csv('taka_remove.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a418cb-6c96-478b-ba61-5b63246a58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def clean_text(text):\n",
    "    # 이모지 제거\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    # 줄바꿈 → 공백\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "    # 숫자 포함\n",
    "    text = re.sub(r'\\S*\\d\\S*', '', text)\n",
    "\n",
    "    # 한자 제거\n",
    "    text = re.sub(r'#\\S*[\\u4E00-\\u9FFF]\\S*', '', text)\n",
    "\n",
    "    # 일본어 거거\n",
    "    text = re.sub(r'#\\S*[\\u3040-\\u309F\\u30A0-\\u30FF]\\S*', '', text)\n",
    "\n",
    "    # 한자/일본어 제거\n",
    "    text = re.sub(r'[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]', '', text)\n",
    "\n",
    "    # 특수문자 제거 \n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)\n",
    "\n",
    "    # 여러 공백을 하나로, 양쪽 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "data_taka['caption_clean'] = data_taka['caption'].apply(lambda x: clean_text(x) if isinstance(x, str) else '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e04183-11f4-43d1-aa04-b4e29731f24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 246 entries, 0 to 245\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   url            246 non-null    object \n",
      " 1   caption        246 non-null    object \n",
      " 2   hashtags       213 non-null    object \n",
      " 3   likes          240 non-null    float64\n",
      " 4   image_url      246 non-null    object \n",
      " 5   caption_clean  246 non-null    object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 11.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data_taka.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48e2a7d5-a23b-4307-af18-5050b5e684d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_tongyeong=pd.read_csv('tongyeong.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df54f5a-e9dd-474a-8987-4ecbd05f8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def clean_text(text):\n",
    "    # 이모지 제거\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    # 줄바꿈 → 공백\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "    # 숫자 포함\n",
    "    text = re.sub(r'\\S*\\d\\S*', '', text)\n",
    "\n",
    "    # 한자 제거\n",
    "    text = re.sub(r'#\\S*[\\u4E00-\\u9FFF]\\S*', '', text)\n",
    "\n",
    "    # 일본어 거거\n",
    "    text = re.sub(r'#\\S*[\\u3040-\\u309F\\u30A0-\\u30FF]\\S*', '', text)\n",
    "\n",
    "    # 한자/일본어 제거\n",
    "    text = re.sub(r'[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF]', '', text)\n",
    "\n",
    "    # 특수문자 제거 \n",
    "    text = re.sub(r'[^\\w\\s#]', '', text)\n",
    "\n",
    "    # 여러 공백을 하나로, 양쪽 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "data_tongyeong['caption_clean'] = data_tongyeong['caption'].apply(lambda x: clean_text(x) if isinstance(x, str) else '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa178510-2695-47c1-8451-433a693b0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################    KoBERT     #####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cb4c92a-7f8c-4397-9aa1-0bf2ba29e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁안', '녕', '하세요', ',', '▁K', 'o', 'B', 'ER', 'T', '▁테스트', '▁중', '입니다', '.']\n",
      "Last Hidden State shape: torch.Size([1, 15, 768])\n",
      "Pooler Output shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from kobert_transformers import get_tokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# 문장\n",
    "sentence = \"안녕하세요, KoBERT 테스트 중입니다.\"\n",
    "\n",
    "# 4. 토큰화 출력\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 5. 토큰을 모델 입력 형태로 변환\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# 6. 모델에 입력 후 출력 받기\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(\"Last Hidden State shape:\", outputs.last_hidden_state.shape)\n",
    "print(\"Pooler Output shape:\", outputs.pooler_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dadc2d8-e677-40a6-a01d-67de0d423cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.54.1\n",
      "0.6.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import kobert_transformers\n",
    "print(transformers.__version__)  # 4.54.1 이상이면 충분함\n",
    "print(kobert_transformers.__version__)  # 0.6.0 이상이면 충분함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cb96a6d-c683-4796-b3ef-cd468db8a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'f', 'get_distilkobert_lm', 'get_distilkobert_model', 'get_kobert_lm', 'get_kobert_model', 'get_tokenizer', 'load_model', 'os', 'tokenization_kobert', 'utils', 'version_txt']\n"
     ]
    }
   ],
   "source": [
    "import kobert_transformers\n",
    "print(dir(kobert_transformers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebe4ccce-d9e7-44e3-95d0-b4662ccd62e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁안', '녕', '하세요', ',', '▁K', 'o', 'B', 'ER', 'T', '▁테스트', '▁중', '입니다', '.']\n",
      "Last Hidden State shape: torch.Size([1, 15, 768])\n",
      "Pooler Output shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from kobert_transformers import get_tokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# 문장\n",
    "sentence = \"안녕하세요, KoBERT 테스트 중입니다.\"\n",
    "\n",
    "# 4. 토큰화 출력\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 5. 토큰을 모델 입력 형태로 변환\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# 6. 모델에 입력 후 출력 받기\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(\"Last Hidden State shape:\", outputs.last_hidden_state.shape)\n",
    "print(\"Pooler Output shape:\", outputs.pooler_output.shape)\n",
    "\n",
    "# last_hidden_state : 문장 내 각 토큰의 문맥적 의미 임베딩\n",
    "# pooler_output : 문장 전체를 대표하는 임베딩 벡터, [cLS]토큰의 마지막 hidden state에 tanh활성화 함수를 적용한 값\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0e7c08-6cdc-43d9-9e78-a6d4f81cb9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############kobert 사용하기 직접"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f7b1ad51-913e-42ad-abbd-c2dba86e4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sentence) for sentence in data_taka['caption_clean']]\n",
    "data_taka[\"tokens\"] = data_taka['caption_clean'].apply(lambda x: tokenizer.tokenize(x))\n",
    "data_taka.to_csv(\"taka_tokenized_check_and_remove.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "data_taka.drop(columns=[\"tokens\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "07d1f325-5b16-470a-8a49-50e1fb9adcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm --quiet\n",
    "!pip install ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c674bf51-febd-4907-81a8-c3626894406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############example##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6e14668d-c4b0-46bf-adbc-28db8a9dda87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two documents: 0.7597843408584595\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# KoBERT 모델과 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# 문장을 임베딩으로 변환하는 함수\n",
    "def get_embedding(text):\n",
    "    # 텍스트를 토큰화\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    \n",
    "    # 모델을 통해 임베딩 계산\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 임베딩은 마지막 hidden state를 사용\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # 평균으로 문서 임베딩을 구함\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# 두 문서 예시\n",
    "doc1 = \"오늘 날씨가 너무 좋다.\"\n",
    "doc2 = \"오늘은 날씨가 맑고 기분이 좋다.\"\n",
    "\n",
    "# 각 문서의 임베딩을 구함\n",
    "embedding1 = get_embedding(doc1)\n",
    "embedding2 = get_embedding(doc2)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "cos_sim = cosine_similarity(embedding1.numpy(), embedding2.numpy())\n",
    "print(f\"Cosine similarity between the two documents: {cos_sim[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf8a7873-e8cd-46d4-8531-b138fce5bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#글이 길면 토큰이 잘릴 수 있으므로 문장 요약을 하거나, \n",
    "#글의 길이나 좋아요 수 등을 고려하여 각 글에 가중치를 부여할 수 도 있다.\n",
    "#250개의 글에 대해 한 번에 bert임베딩을 하면 메모리 사용량이 너무 많을 수 있으니 이때는 배치(batch)처리로 나누어서 임베딩 구할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "15b92d15-4ce7-4128-9874-8eae91a1e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [0.015660496, -0.12379188, 0.29436022, -3.8897...\n",
      "1    [-0.023761913, -0.121277794, 0.40451685, -5.30...\n",
      "2    [0.004951356, -0.09263014, 0.20708022, -3.7605...\n",
      "3    [-0.065095134, -0.218861, 0.075367354, -3.2439...\n",
      "4    [0.09418768, -0.18728587, 0.19061619, -3.62158...\n",
      "Name: embeddings, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from kobert_transformers import get_tokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    #평균 풀링: 모든 토큰의 표현에 평균 풀링으로 문장 표현을 얻으면 문장 표현은 본질적으로 모든 단어(토큰)의 의미를 가짐\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "    \n",
    "def add_embeddings_to_dataframe(data_taka):\n",
    "    embeddings = []\n",
    "    for text in data_taka['caption_clean']:\n",
    "        emb = get_embedding(text)\n",
    "        embeddings.append(emb.squeeze().numpy())  # 텐서를 numpy 배열로 변환 후 리스트에 추가\n",
    "    \n",
    "    # 새 컬럼 'embedding'으로 임베딩 값 추가\n",
    "    data_taka['embeddings'] = embeddings\n",
    "    return data_taka\n",
    "\n",
    "# DataFrame에 임베딩 추가\n",
    "data_with_embeddings = add_embeddings_to_dataframe(data_taka)\n",
    "\n",
    "# 결과 출력\n",
    "print(data_taka['embeddings'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb7572-f993-4d00-b0bf-e5bebc532f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_transformers import get_tokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    #평균 풀링: 모든 토큰의 표현에 평균 풀링으로 문장 표현을 얻으면 문장 표현은 본질적으로 모든 단어(토큰)의 의미를 가짐\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b06f1a00-cee7-4ae9-a8b2-c87fd413b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "def extract_nouns(text):\n",
    "    # Okt를 사용해 명사만 추출\n",
    "    nouns = okt.nouns(text)\n",
    "    # 한 단어로 이루어진 명사는 삭제\n",
    "    filtered_nouns = [noun for noun in nouns if len(noun) > 1]\n",
    "    return ' '.join(filtered_nouns)\n",
    "\n",
    "\n",
    "data_taka['caption_nouns'] = data_taka['caption_clean'].apply(extract_nouns)\n",
    "result_taka = ' '.join(data_taka['caption_nouns'])\n",
    "\n",
    "data_tongyeong['caption_nouns'] = data_tongyeong['caption_clean'].apply(extract_nouns)\n",
    "result_tongyeong= ' '.join(data_tongyeong['caption_nouns'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7c0844-12fb-4ecb-9fbb-8e41ce1d988e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 Nouns in Taka:\n",
      "우동: 363\n",
      "다카마쓰: 234\n",
      "여행: 202\n",
      "일본: 165\n",
      "공원: 68\n",
      "시간: 54\n",
      "다카: 54\n",
      "소도시: 52\n",
      "야돈: 51\n",
      "사진: 49\n",
      "마츠: 44\n",
      "투어: 42\n",
      "예약: 39\n",
      "시코쿠: 35\n",
      "호텔: 35\n",
      "추천: 34\n",
      "예술: 34\n",
      "도시: 33\n",
      "미술관: 31\n",
      "맛집: 30\n",
      "온천: 28\n",
      "진짜: 26\n",
      "하나: 26\n",
      "숙소: 24\n",
      "버스: 24\n",
      "방문: 23\n",
      "생각: 22\n",
      "튀김: 22\n",
      "카페: 22\n",
      "사람: 21\n",
      "정말: 21\n",
      "가장: 21\n",
      "마루: 21\n",
      "도시마: 21\n",
      "공항: 19\n",
      "이번: 19\n",
      "정원: 19\n",
      "하루: 19\n",
      "힐링: 18\n",
      "카가와현: 18\n",
      "마치: 18\n",
      "자전거: 18\n",
      "작품: 18\n",
      "우리: 18\n",
      "바로: 17\n",
      "여기: 17\n",
      "가게: 17\n",
      "포함: 17\n",
      "사누키: 17\n",
      "올리브: 17\n",
      "\n",
      "Top 50 Nouns in Tongyeong:\n",
      "통영: 875\n",
      "여행: 203\n",
      "통영시: 198\n",
      "경남: 157\n",
      "카페: 154\n",
      "바다: 142\n",
      "맛집: 109\n",
      "여름: 88\n",
      "케이크: 73\n",
      "추천: 72\n",
      "시간: 71\n",
      "거제: 71\n",
      "디저트: 63\n",
      "공원: 54\n",
      "마을: 52\n",
      "수국: 50\n",
      "힐링: 45\n",
      "코스: 44\n",
      "축제: 42\n",
      "펜션: 42\n",
      "고양이: 41\n",
      "고성: 39\n",
      "투어: 38\n",
      "벽화: 38\n",
      "비진도: 38\n",
      "행사: 37\n",
      "마음: 36\n",
      "사진: 36\n",
      "이번: 36\n",
      "한산: 35\n",
      "풍경: 35\n",
      "방문: 35\n",
      "휴가: 34\n",
      "여행지: 33\n",
      "욕지도: 33\n",
      "자연: 32\n",
      "필수: 32\n",
      "확인: 32\n",
      "사람: 31\n",
      "체험: 31\n",
      "장소: 31\n",
      "여유: 31\n",
      "가능: 31\n",
      "오션: 30\n",
      "도시: 29\n",
      "하나: 29\n",
      "소개: 29\n",
      "요트: 28\n",
      "준비: 28\n",
      "바로: 28\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 결과 텍스트에서 명사들을 추출\n",
    "all_nouns_taka = ' '.join(data_taka['caption_nouns']).split()\n",
    "all_nouns_tongyeong= ' '.join(data_tongyeong['caption_nouns']).split()\n",
    "\n",
    "# 명사들의 빈도 계산\n",
    "counter_taka = Counter(all_nouns_taka)\n",
    "counter_tongyeong= Counter(all_nouns_tongyeong)\n",
    "\n",
    "# 상위 50개의 명사 추출\n",
    "top_50_taka = counter_taka.most_common(50)\n",
    "top_50_tongyeong = counter_tongyeong.most_common(50)\n",
    "\n",
    "# 상위 50개 명사 출력\n",
    "print(\"Top 50 Nouns in Taka:\")\n",
    "for noun, freq in top_50_taka:\n",
    "    print(f\"{noun}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 50 Nouns in Tongyeong:\")\n",
    "for noun, freq in top_50_tongyeong:\n",
    "    print(f\"{noun}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d062db3f-e6f4-4f7b-a954-49da34d2b61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 텍스트의 유사도: 0.9027\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from kobert_transformers import get_tokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# 텍스트를 BERT에 맞게 토큰화하고, 임베딩을 추출하는 함수\n",
    "def embed_text(text):\n",
    "    # 텍스트를 토큰화 (각 단어를 BERT 입력 형식으로 변환)\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # BERT 모델을 사용해 임베딩 추출\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # 문장의 임베딩은 평균값으로 취합\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# 텍스트 임베딩\n",
    "embedding_taka = embed_text(result_taka)\n",
    "embedding_tongyeong= embed_text(result_tongyeong)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = cosine_similarity(embedding_taka.numpy(), embedding_tongyeong.numpy())\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"두 텍스트의 유사도: {similarity[0][0]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
