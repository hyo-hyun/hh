{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31885701-04ab-4a22-9c94-734468b26783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\myroom\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:/myroom/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85fd1df5-9188-4712-af60-963a5aebb306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 핸들링\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#전처리\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#모델\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#하이퍼 파라미터 튜닝: 보통 랜덤서치로 대략 파악한 다음 그리드 서치로 미세조정\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#평가\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c295ac3-86e1-49c5-872f-46c1ce00b29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#pd.set_option('display.max_rows',None)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "train= pd.read_csv(\"./nlp-getting-started/train.csv\")\n",
    "test= pd.read_csv(\"./nlp-getting-started/test.csv\")\n",
    "train.shape, test.shape\n",
    "# ((7613, 5), (3263, 4))\n",
    "\n",
    "train['target'].value_counts()\n",
    "#1:실제 발생, 0: 발생하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c2353e-2fe4-4a8a-9328-7c3bd7af47a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65008d78-8f32-4555-be9d-50c092b62f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64 \n",
      "\n",
      "id 의 결측치 비율: 0.0\n",
      "keyword 의 결측치 비율: 0.008\n",
      "location 의 결측치 비율: 0.333\n",
      "text 의 결측치 비율: 0.0\n",
      "target 의 결측치 비율: 0.0\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "print(train.isnull().sum(),'\\n')\n",
    "for i in range(len(train.columns)):\n",
    "    print(train.columns[i],'의 결측치 비율:', round(train.isnull().sum().iloc[i]/len(train),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c3924e-1e27-47d2-bb49-e1db21629266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id             0\n",
      "keyword       26\n",
      "location    1105\n",
      "text           0\n",
      "dtype: int64 \n",
      "\n",
      "id 의 결측치 비율: 0.0\n",
      "keyword 의 결측치 비율: 0.008\n",
      "location 의 결측치 비율: 0.339\n",
      "text 의 결측치 비율: 0.0\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "print(test.isnull().sum(),'\\n')\n",
    "for i in range(len(test.columns)):\n",
    "    print(test.columns[i],'의 결측치 비율:', round(test.isnull().sum().iloc[i]/len(test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab88c1c5-137d-48c6-8022-51a119cbf146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id             0\n",
      "keyword        0\n",
      "location    2472\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "학습데이터의 location unique한 값의 종류: 3341\n",
      "학습 데이터의 크기 5080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7552"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.dropna(subset=['keyword'],axis=0) # 결측치 비율이 0.008로 큰 의미없다고 판단 =>제거\n",
    "\n",
    "print(train.isnull().sum())\n",
    "\n",
    "\n",
    "#아는 것\n",
    "X_known=train[train['location'].notna()]\n",
    "\n",
    "#모르는 것\n",
    "X_unknown=train[train['location'].isnull()].drop('location',axis=1)\n",
    "\n",
    "print('\\n')\n",
    "print('학습데이터의 location unique한 값의 종류:', X_known['location'].nunique()) #3341\n",
    "print('학습 데이터의 크기',len(X_known))\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4a03fe8-7256-4d0e-b1b9-8f55d6a3ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드별 재난트윗일 확률 평균 계산\n",
    "\n",
    "kw_mean1 = X_known.groupby('keyword')['target'].mean().sort_values()\n",
    "risk_map1 = kw_mean1.to_dict()\n",
    "X_known['keyword_score'] = X_known['keyword'].map(risk_map1)\n",
    "kw_score1 =X_known['keyword_score']\n",
    "\n",
    "text1 = X_known['text']\n",
    "\n",
    "x_df = pd.DataFrame({'text':text1,'kw_score':kw_score1})\n",
    "\n",
    "## 장소 20개만\n",
    "top20= X_known['location'].value_counts().index[:20]\n",
    "X_known['location']=X_known['location'].mask(~X_known['location'].isin(top20),'etc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6fea459-0a61-44a0-9b4e-90812c97e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_mean2 = X_unknown.groupby('keyword')['target'].mean().sort_values()\n",
    "risk_map2 = kw_mean2.to_dict()\n",
    "X_unknown['keyword_score'] = X_unknown['keyword'].map(risk_map2)\n",
    "kw_score2 =X_unknown['keyword_score']\n",
    "\n",
    "text2 = X_unknown['text']\n",
    "\n",
    "x_target_df = pd.DataFrame({'text':text2,'kw_score':kw_score2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1f083a8-da0b-4128-ad95-fe04196a1f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>kw_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>on the outside you're ablaze and alive\\nbut yo...</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SOOOO PUMPED FOR ABLAZE ???? @southridgelife</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I wanted to set Chicago ablaze with my preachi...</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>I gained 3 followers in the last week. You? Kn...</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>Wrecked tired but not gonna be asleep before 3??</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>The Riddler would be the best early-exit prima...</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7576</th>\n",
       "      <td>He just wrecked all of you http://t.co/y46isyZkC8</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582</th>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2472 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  kw_score\n",
       "38    Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...  0.142857\n",
       "41    on the outside you're ablaze and alive\\nbut yo...  0.142857\n",
       "43         SOOOO PUMPED FOR ABLAZE ???? @southridgelife  0.142857\n",
       "44    I wanted to set Chicago ablaze with my preachi...  0.142857\n",
       "45    I gained 3 followers in the last week. You? Kn...  0.142857\n",
       "...                                                 ...       ...\n",
       "7565   Wrecked tired but not gonna be asleep before 3??  0.058824\n",
       "7568  The Riddler would be the best early-exit prima...  0.058824\n",
       "7576  He just wrecked all of you http://t.co/y46isyZkC8  0.058824\n",
       "7578   @jt_ruff23 @cameronhacker and I wrecked you both  0.058824\n",
       "7582  Cramer: Iger's 3 words that wrecked Disney's s...  0.058824\n",
       "\n",
       "[2472 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "140469a0-5778-49b6-8c63-543858438ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. 트위터 특화 BERT 로드\n",
    "model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 2. 문장 리스트를 CLS 임베딩으로 벡터화하는 함수\n",
    "def get_cls_batch(texts, batch_size=16, max_len=128):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "    dataloader = DataLoader(texts, batch_size=batch_size)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # 트윗 전처리\n",
    "        batch = [text.replace(\"http://\", \"\").replace(\"https://\", \"\").strip() for text in batch]\n",
    "\n",
    "        # 토크나이징\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :] \n",
    "        all_embeddings.append(cls_embeddings.cpu().numpy())  \n",
    "\n",
    "    return np.vstack(all_embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d2749fd-02dc-4353-9dd9-ae04f01423d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5080, 769)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36609a7c-244b-4f3f-b1d5-8dbadd1085c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = get_cls_batch(x_df['text'].tolist())\n",
    "vector2 = get_cls_batch(x_target_df['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "411057f5-18ac-4bb8-87e2-0d9d10d3c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1=StandardScaler().fit_transform(vector1)\n",
    "v2=StandardScaler().fit_transform(pd.DataFrame(x_df['kw_score']))\n",
    "\n",
    "v3=StandardScaler().fit_transform(vector2)\n",
    "v4=StandardScaler().fit_transform(pd.DataFrame(x_target_df['kw_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6bd2727-17a1-43c3-9d54-f68489ea5ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터의 크기: (5080, 769)\n",
      "타겟의 크기: (2472, 769)\n"
     ]
    }
   ],
   "source": [
    "#데이터 병합\n",
    "x_df = pd.DataFrame(np.concatenate([v1, v2], axis=1), index=X_known.index)\n",
    "print('학습데이터의 크기:',x_df.shape)\n",
    "\n",
    "x_target_df = pd.DataFrame(np.concatenate([v3, v4], axis=1), index=X_unknown.index)\n",
    "print('타겟의 크기:',x_target_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ba07eca-29d2-4557-b6e2-07f58b579aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.900682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.901609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.902523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.903425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.904321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "159  0.900682\n",
       "160  0.901609\n",
       "161  0.902523\n",
       "162  0.903425\n",
       "163  0.904321\n",
       "..        ...\n",
       "764  0.999998\n",
       "765  0.999999\n",
       "766  0.999999\n",
       "767  1.000000\n",
       "768  1.000000\n",
       "\n",
       "[610 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습데이터의 주성분 분석\n",
    "p=pd.DataFrame(np.cumsum(PCA().fit(x_df).explained_variance_ratio_))\n",
    "p[p.iloc[:,0]>=0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa2d7bcf-e33b-4602-a0b3-a2a32607614b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.900683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.901670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.902651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.903612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.904565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>618 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "151  0.900683\n",
       "152  0.901670\n",
       "153  0.902651\n",
       "154  0.903612\n",
       "155  0.904565\n",
       "..        ...\n",
       "764  0.999998\n",
       "765  0.999999\n",
       "766  1.000000\n",
       "767  1.000000\n",
       "768  1.000000\n",
       "\n",
       "[618 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#타겟의 주성분 분석\n",
    "p=pd.DataFrame(np.cumsum(PCA().fit(x_target_df).explained_variance_ratio_))\n",
    "p[p.iloc[:,0]>=0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "555c7507-c3c7-4d99-85cd-860b3412c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = PCA(n_components=159).fit_transform(x_df)\n",
    "x_data = pd.DataFrame(x_data,index=X_known.index)\n",
    "x_target = PCA(n_components=159).fit_transform(x_target_df)\n",
    "x_target = pd.DataFrame(x_target,index=X_unknown.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "409b606b-ee73-4b65-8561-06eba5fcbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y=X_known['location'] \n",
    "lb=LabelEncoder()\n",
    "y=lb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "563ad2b2-6739-4a8e-8643-4b62c5206866",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train, y_test = train_test_split(x_df,y,test_size=0.3,random_state=123)\n",
    "x_train,x_val,y_train, y_val = train_test_split(x_train,y_train,test_size=0.15,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16c51035-ed6e-4c90-8190-77ffbb524929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.198 => 과적합\n",
      "f1 score = 0.816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DTCmodel=DecisionTreeClassifier(random_state=123).fit(x_train,y_train)\n",
    "y_pred5=DTCmodel.predict(x_test)\n",
    "\n",
    "train_score=DTCmodel.score(x_train,y_train)\n",
    "validation_score=DTCmodel.score(x_val,y_val)\n",
    "\n",
    "differ=round((train_score-validation_score),3)\n",
    "\n",
    "\n",
    "if not 0<=differ<=0.05:\n",
    "    print(differ,'=> 과적합')\n",
    "else:\n",
    "    print(differ,'=> 과적합 아님')\n",
    "\n",
    "\n",
    "print('f1 score =',round(f1_score(y_test,y_pred5,average='weighted'),3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74d53403-5a2f-4725-bb62-91f96fedfc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV best params: {'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 5, 'criterion': 'gini'}\n",
      "GridSearchCV best params: {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "RandomizedSearchCV 최적 파라미터로 학습한 모델 F1-score: 0.853\n",
      "GridSearchCV 최적 파라미터로 학습한 모델 F1-score: 0.839\n",
      "RandomizedSearchCV 차이 (0.001) => 과적합 아님\n",
      "GridSearchCV 차이 (0.001) => 과적합 아님\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 의사결정트리 모델을 위한 하이퍼파라미터 범위 설정\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # 분할 기준\n",
    "    'max_depth': [5, 10, 15, 20, None],  # 트리의 최대 깊이\n",
    "    'min_samples_split': [2, 5, 10, 20],  # 분할을 위한 최소 샘플 수\n",
    "    'min_samples_leaf': [1, 2, 4, 10],  # 리프 노드에서 최소 샘플 수\n",
    "    'max_features': [None, 'sqrt', 'log2'],  # 분할 시 고려할 최대 특성 수\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV로 대략적인 최적 파라미터를 찾기\n",
    "g1 = RandomizedSearchCV(DecisionTreeClassifier(random_state=123), param_distributions=param_grid,\n",
    "                        n_iter=50, cv=5, random_state=42, n_jobs=-1)\n",
    "g1.fit(x_train, y_train)\n",
    "\n",
    "# GridSearchCV로 최적의 파라미터를 세밀하게 조정\n",
    "g2 = GridSearchCV(DecisionTreeClassifier(random_state=123), param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "g2.fit(x_train, y_train)\n",
    "\n",
    "# 최적의 파라미터 출력\n",
    "print(\"RandomizedSearchCV best params:\", g1.best_params_)\n",
    "print(\"GridSearchCV best params:\", g2.best_params_)\n",
    "\n",
    "# 최적 파라미터로 모델 학습\n",
    "best_params_randomized = g1.best_params_\n",
    "best_params_grid = g2.best_params_\n",
    "\n",
    "# RandomizedSearchCV 결과로 모델 학습\n",
    "final_model_randomized = DecisionTreeClassifier(random_state=42, **best_params_randomized)\n",
    "final_model_randomized.fit(x_train, y_train)\n",
    "\n",
    "# GridSearchCV 결과로 모델 학습\n",
    "final_model_grid = DecisionTreeClassifier(random_state=42, **best_params_grid)\n",
    "final_model_grid.fit(x_train, y_train)\n",
    "\n",
    "# 예측 수행\n",
    "y_pred_randomized = final_model_randomized.predict(x_test)\n",
    "y_pred_grid = final_model_grid.predict(x_test)\n",
    "\n",
    "# F1-score 계산\n",
    "f1_randomized = f1_score(y_test, y_pred_randomized, average='weighted')\n",
    "f1_grid = f1_score(y_test, y_pred_grid, average='weighted')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"RandomizedSearchCV 최적 파라미터로 학습한 모델 F1-score:\", round(f1_randomized, 3))\n",
    "print(\"GridSearchCV 최적 파라미터로 학습한 모델 F1-score:\", round(f1_grid, 3))\n",
    "\n",
    "# 훈련 데이터와 검증 데이터 점수 차이 계산 (과적합 확인)\n",
    "train_score_randomized = final_model_randomized.score(x_train, y_train)\n",
    "validation_score_randomized = final_model_randomized.score(x_val, y_val)\n",
    "\n",
    "train_score_grid = final_model_grid.score(x_train, y_train)\n",
    "validation_score_grid = final_model_grid.score(x_val, y_val)\n",
    "\n",
    "# 점수 차이 계산\n",
    "differ_randomized = round((train_score_randomized - validation_score_randomized), 3)\n",
    "differ_grid = round((train_score_grid - validation_score_grid), 3)\n",
    "\n",
    "# 과적합 여부 확인\n",
    "if not 0 <= differ_randomized <= 0.05:\n",
    "    print(f\"RandomizedSearchCV 차이 ({differ_randomized}) => 과적합\")\n",
    "else:\n",
    "    print(f\"RandomizedSearchCV 차이 ({differ_randomized}) => 과적합 아님\")\n",
    "\n",
    "if not 0 <= differ_grid <= 0.05:\n",
    "    print(f\"GridSearchCV 차이 ({differ_grid}) => 과적합\")\n",
    "else:\n",
    "    print(f\"GridSearchCV 차이 ({differ_grid}) => 과적합 아님\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
